- name: coyote
  title: kafka-connect-oracle

- name: Setup Containers
  entries:
    - name: Docker Compose Pull
      command: |
       docker-compose -f docker-compose-all.yml -p kafkaconnectoracle pull
      ignore_exit_code: true
      stdout_not_has: [ 'error' ]
    - name: Docker Compose Up
      command: docker-compose -f docker-compose-all.yml -p kafkaconnectoracle up -d
    - name: Wait for Connect to get up
      command: |
        bash -c '
          for ((i=0;i<30;i++)); do
            sleep 10;
            docker exec connect curl "http://localhost:8083/connectors/" && break;
          done'
      nolog: false
- name: Setup Oracle Connectors
  entries:
    - name: Create a Increment Distributed Connector
      command: |
        curl -vs --stderr - -X POST -H "Content-Type: application/json"
             --data @-
             "http://localhost:8083/connectors"
      stdin: |
        {
            "name": "aws-rumo-integ-source-translogic-tfa-incrementing-1",
            "config": {
                "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
                "connection.url": "jdbc:oracle:thin:@10.200.92.119:1521:tlfd",
                "connection.user": "translogic",
                "connection.password": "engesis",
                "topic.prefix": "translogic-1-vw-prisma-tfa-kakfa",
                "mode": "incrementing",
                "incrementing.column.name": "NUM_PROCESSO",
                "poll.interval.ms": 300000,
                "tasks.max": "1",
                "schema.pattern": "translogic",
                "query": "SELECT ATIVO, BASE_CALCULO_ICMS, BITOLA, CAUSA, CODIGO_FLUXO, DATA_CTE, DATA_DESPACHO, DATA_INCLUSAO, DATA_SINISTRO, DATA_VISTORIA, DESC_POR_PESO, DESCRICAO_MERCADORIA, DESPACHO, FRETE, GAMBITAGEM, HISTORICO, ID_CLIENTE_CORRENTISTA, ID_CLIENTE_DESTINATARIO, ID_CLIENTE_REMETENTE, ID_DESPACHO, ID_MERCADORIA, ID_SINDICANCIA, ID_UNIDADE_NEGOCIO, ID_VAGAO, LACRE, LAUDO, MALHA, MATRICULA_INCLUSAO, NOME_CLIENTE_CORRENTISTA, NOME_CLIENTE_DESTINATARIO, NOME_CLIENTE_REMETENTE, NOME_ESTACAO, NOME_ESTACAO_DESTINO, NOME_ESTACAO_ORIGEM, NOME_INCLUSAO, NOME_RES_CLIENTE_CORRENTISTA, NOME_RES_CLIENTE_DESTINATARIO, NOME_RES_CLIENTE_REMETENTE, NOME_TERMINAL_DESTINO, NOME_TERMINAL_ORIGEM, NOME_VISTORIADOR, NOTA_FISCAL, NUMERO_CTE, CAST(NUM_PROCESSO AS INTEGER) NUM_PROCESSO, ORIGEM, PERDA, PESO_CALCULO, PESO_CTE, PESO_TOTAL_NOTA_FISCAL, PLACA_VAGAO, RATEIO, SERIE_CTE, SERIE_DESPACHO, SERIE_VAGAO, SIGLA_ESTACAO, SIGLA_ESTACAO_DESTINO, SIGLA_ESTACAO_ORIGEM, SIGLA_TERMINAL_DESTINO, SIGLA_TERMINAL_ORIGEM, TOLERANCIA, UNIDADE_MEDIDA, UNIDADE_NEGOCIO, UNIDADE_PRODUCAO, VALOR_FRETE, VALOR_ICMS, VALOR_PAGAR, VALOR_TOTAL_CTE, VALOR_TOTAL_MERCADORIA, VALOR_TOTAL_NOTA_FISCAL, VALOR_UNITARIO FROM VW_PRISMA_TFA_KAFKA_INCREMENT",
                "validate.non.null": "false",
                "locale":"pt-BR",
                "quote.sql.identifiers": "never"
            }
        }
      stdout_has: ["HTTP/1.1 [2][0-9][0-9] "]  
      stdout_not_has: ["HTTP/1.1 [45][0-9][0-9] "] 
    - name: Create a Timestamp Distributed Connector
      command: |
        curl -vs --stderr - -X POST -H "Content-Type: application/json"
             --data @-
             "http://localhost:8083/connectors"
      stdin: |
             {
               "name": "aws-rumo-integ-source-translogic-tfa-timestamp-1",
               "config": {
                 "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
                 "connection.url": "jdbc:oracle:thin:@10.200.92.119:1521:tlfd",
                 "connection.user": "translogic",
                 "connection.password": "engesis",
                 "topic.prefix": "translogic-1-vw-prisma-tfa-kakfa",
                 "mode": "timestamp",
                 "timestamp.column.name": "DATA_ALTERACAO",
                 "poll.interval.ms": 300000,
                 "tasks.max": "1",
                 "schema.pattern": "translogic",
                 "query": "SELECT ATIVO, BASE_CALCULO_ICMS, BITOLA, CAUSA, CODIGO_FLUXO, DATA_ALTERACAO, DATA_CTE, DATA_DESPACHO, DATA_INCLUSAO, DATA_SINISTRO, DATA_VISTORIA, DESC_POR_PESO, DESCRICAO_MERCADORIA, DESPACHO, FRETE, GAMBITAGEM, HISTORICO, ID_CLIENTE_CORRENTISTA, ID_CLIENTE_DESTINATARIO, ID_CLIENTE_REMETENTE, ID_DESPACHO, ID_MERCADORIA, ID_SINDICANCIA, ID_UNIDADE_NEGOCIO, ID_VAGAO, LACRE, LAUDO, MALHA, MATRICULA_ALTERACAO, MATRICULA_INCLUSAO, NOME_ALTERACAO, NOME_CLIENTE_CORRENTISTA, NOME_CLIENTE_DESTINATARIO, NOME_CLIENTE_REMETENTE, NOME_ESTACAO, NOME_ESTACAO_DESTINO, NOME_ESTACAO_ORIGEM, NOME_INCLUSAO, NOME_RES_CLIENTE_CORRENTISTA, NOME_RES_CLIENTE_DESTINATARIO, NOME_RES_CLIENTE_REMETENTE, NOME_TERMINAL_DESTINO, NOME_TERMINAL_ORIGEM, NOME_VISTORIADOR, NOTA_FISCAL, NUMERO_CTE, CAST(NUM_PROCESSO AS INTEGER) NUM_PROCESSO, ORIGEM, PERDA, PESO_CALCULO, PESO_CTE, PESO_TOTAL_NOTA_FISCAL, PLACA_VAGAO, RATEIO, SERIE_CTE, SERIE_DESPACHO, SERIE_VAGAO, SIGLA_ESTACAO, SIGLA_ESTACAO_DESTINO, SIGLA_ESTACAO_ORIGEM, SIGLA_TERMINAL_DESTINO, SIGLA_TERMINAL_ORIGEM, TOLERANCIA, UNIDADE_MEDIDA, UNIDADE_NEGOCIO, UNIDADE_PRODUCAO, VALOR_FRETE, VALOR_ICMS, VALOR_PAGAR, VALOR_TOTAL_CTE, VALOR_TOTAL_MERCADORIA, VALOR_TOTAL_NOTA_FISCAL, VALOR_UNITARIO FROM VW_PRISMA_TFA_KAFKA_TIMESTAMP",
                 "validate.non.null": "false",
                 "locale":"pt-BR",
                 "quote.sql.identifiers": "never"
                }
              }
      stdout_has: ["HTTP/1.1 [2][0-9][0-9] "]  
      stdout_not_has: ["HTTP/1.1 [45][0-9][0-9] "]
    - name: Create a Sink Distributed Connector
      command: |
        curl -vs --stderr - -X POST -H "Content-Type: application/json"
           --data @- "http://localhost:8083/connectors"
      stdin: |
       {
         "name": "aws-rumo-integ-sink-tfa-prisma-0",
         "config": {
            "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
            "tasks.max": "1",
            "topics": "translogic-1-vw-prisma-tfa-kakfa",
            "batch.size": "10",
            "connection.url": "jdbc:oracle:thin:@10.200.92.119:1521:DEV33",
            "connection.user": "controle_perdas",
            "connection.password": "dSWARJSaTUxF",
            "table.name.format": "TFA",
            "schema.pattern": "controle_perdas",
            "insert.mode": "upsert",
            "pk.mode": "record_value",
            "pk.fields": "NUM_PROCESSO",
            "auto.create": "false",
            "auto.evolve": "false"
         } 
       }
      stdout_has: ["HTTP/1.1 [2][0-9][0-9] "]  
      stdout_not_has: ["HTTP/1.1 [45][0-9][0-9] "]   

# - name: Test Connector
#   entries:
#     - name: Write Entries into Topic
#       command: >
#         docker run --rm -i --network=kafkaconnectvoltdb_default landoop/fast-data-dev
#           kafka-avro-console-producer --broker-list fast-data-dev:9092
#             --topic voltdb-sink --property schema.registry.url="http://fast-data-dev:8081"
#             --property
#             value.schema='{"type":"record","name":"User","namespace":"com.datamountaineer.streamreactor.connect.voltdb","fields":[{"name":"firstname","type":"string"},{"name":"lastname","type":"string"},{"name":"age","type":"int"},{"name":"salary","type":"double"}]}'
#       stdin: |
#         {"firstname": "John", "lastname": "Smith", "age":30, "salary": 4830}
#         {"firstname": "Anna", "lastname": "Doe", "age":30, "salary": 5820}
#       timeout: 20s
#     - command: sleep 30
#       nolog: true
#     - name: Verify entries in VoltDB
#       command: >
#         docker exec -i kafkaconnectvoltdb_voltdb_1
#           sqlcmd
#       stdin: |
#         SELECT * FROM person;
#       stdout_has: ["John", "Smith", "Anna", "Doe", "5820"]
#     - name: Read First 4000 Lines of Connect Logs
#       command: >
#         docker exec kafkaconnectvoltdb_fast-data-dev_1 head -n4000 /var/log/connect-distributed.log
#       stdout_not_has: ['\] ERROR']

- name: Clean-up Containers
  entries:
    - name: Docker Compose Down
      command: |
        docker-compose -f docker-compose-all.yml -p kafkaconnectoracle down
      stdout_not_has: ['error']
